<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[123]]></title>
    <url>%2F2019%2F11%2F22%2F123%2F</url>
    <content type="text"><![CDATA[2132132]]></content>
  </entry>
  <entry>
    <title><![CDATA[uwsgi+nginx+阿里云部署django项目]]></title>
    <url>%2F2019%2F11%2F02%2Fuwsgi-nginx-%E9%98%BF%E9%87%8C%E4%BA%91%E9%83%A8%E7%BD%B2django%E9%A1%B9%E7%9B%AE%2F</url>
    <content type="text"><![CDATA[写在前头我用到的配置：python版本：3.6服务器：阿里云Ubuntu16.04 这篇文章只涉及部署内容，不涉及具体django代码，且我在部署时使用的时创建django项目时使用的是默认的SQLite数据库，使用MySQL,MongoDB或者其他数据的大佬们请自行下载配置。由于在我部署时python3.7有些不稳定所以改用python3.6版本(python3.8如今也出来了)。 由于不涉及django代码，所以请大家将自己要部署的django项目上传到GitHub上(Gitee)，以方便在部署时直接克隆项目。 部署前的python环境配置由于Linux一般自带python2.x版本，而咱们现在用的python版本为3.x(大家可以通过python -V来查看python版本)，所以第一步咱们要配置python环境。另外，在部署时咱们要用到虚拟环境(virtualenv)和相关库(django等),所以在虚拟环境里，咱们也得把这配置好。下边实在服务器上的操作： 安装python3.6apt-get install software-properties-commonadd-apt-repository ppa:jonathonf/python-3.6apt-get updateapt-get install python3.6 以上步骤除了在安装时需要按y或者Enter确定安装外，最容易出现的错误是出现一个Unable啥的(即不能安装或安装失败),这个时候执行一下apt-get update来更新一下系统所用的源(个人理解为软件包),再执行其他命令，就完事了。 创建软连接cd /usr/binrm pythonln -s python3.6 pythonrm python3ln -s python3.6 python3 这样做的目的时使python和python3指向python3.6(因为python默认指向python2.x)。个人理解就是你桌面上有英雄联盟的一个快捷方式(链接文件)，它指向英雄联盟的一个可执行文件，即点击这个桌面图标或者它实际指向的文件是同一个效果。这里咱们干的就是这个事。 安装pip3.6apt-get install python3-pippip3 install --upgrade pip 这里可能会出现不能安装的情况，且使用apt-get update似乎也于事无补，但可以通过重新登录的方式解决(退了重进)。还有就是大家的pip可能报错，解决的思路跟上边一样，我把pip给他删了，再让它重新指向pip3.6(同样是在 /usr/bin目录里操作)cd /usr/binrm pipln -s pip3.6 piprm pip3ln -s pip3.6 pip3 安装虚拟环境pip install virtualenv 想必大家肯定熟悉这个语句。 创建虚拟环境这里我所在的目录是 /homevirtualenv WeekSummaryApp_envsource WeekSummaryApp_env/bin/activate 这里虚拟环境的名我是用自己项目的名称 + _env方便操作区分。上边第一条语句为创建虚拟环境，创建后该目录里是有东西的(不必纠结是啥)，接着我们启动虚拟环境(启动后会发现在命令行前边多了一个(WeekSummaryApp_env)) 安装相关库及Git注意：一下操作均在虚拟环境里完成且根据个人所需安装相关库时有所不同。pip install djangopip install djangorestframework…apt-get install git（瞬间打脸，这个也可以不用在虚拟环境里装，但咱就图个快活方便） 克隆项目git clone 项目地址 这个也要在虚拟环境下完成。 使用uwsgi + nginx部署这里有兴趣的童鞋可以去看官方文档,这里主要介绍操作。 安装uwsgiapt-get install libpython3.6-devpip3 install uwsgi 这里值得说的时如果没有libpython3.6，下一步命令是无法成功执行的。 配置uwsgicd /homnemkdir WeekSummaryApp_uwsgicd WeekSummaryApp_uwsgivim WeekSummaryApp.ini 如果在安装uwsgi时出现ModuleNotFoundError: No module named &#39;gdbm&#39;啥的可以执行apt-get install python3-gdbm然后再安装uwsgi即可 WeekSummaryApp.ini里的内容 1234567891011121314151617[uwsgi]chdir = /home/WeekSummaryApphome = /home/WeekSummaryApp_envmodule = WeekSummaryApp.wsgi:applicationmaster = Trueprocesses = 4harakiri = 60max-requests = 5000socket = 127.0.0.1:8001uid = 1000gid = 2000pidfile = /home/WeekSummaryApp_uwsgi/master.piddaemonize = /home/WeekSummaryApp_uwsgi/WeekSummaryApp.logvacuum = True 启动uwsgiuwsgi --ini /home/WeekSummaryApp_uwsgi/WeekSummaryApp.ini 启动完成后会发现在/home/WeekSummaryApp_uwsgi里多了两文件。我们可以通过ps -aux | grep uwsgi来瞅瞅它到底启动没有，如果执行该命令有多行都有uwsgi则启动成功，要是只有一行则可能是代码敲错了(亲身体会) 安装nginxapt-get install nginx nginx配置文件cd /etc/nginx/sites-availablerm default 没有可忽略这一步vim WeekSummaryApp.conf 编写配置文件 WeekSummaryApp是文件名 WeekSummaryApp.conf里的内容 1234567891011121314151617181920server &#123; listen 80; server_name WeekSummaryApp; charset utf-8; client_max_body_size 75M; location /static &#123; alias /home/WeekSummaryApp/static; &#125; location /media &#123; alias /home/WeekSummaryApp/media; &#125; location / &#123; uwsgi_pass 127.0.0.1:8001; include /etc/nginx/uwsgi_params; &#125;&#125; 在sites-enabled创建软连接ln -s /etc/nginx/sites-available/WeekSummaryApp.conf /etc/nginx/sites-enabled/WeekSummaryApp.conf 这是会发现在/etc/nginx/sites-enabled/里多了一个WeekSummaryApp.conf 启动nginx服务nginx -tservice nginx restart 启动完成后就能在公网看到自己的django项目 最后的说明由于这是基于我自己的一个项目来部署的(项目名为WeekSummaryApp)，所以各位部署自己的项目时将它改成自己项目名就好了。 由于我的这个项目使用的是SQLite数据库(这个数据库是只读数据库，即readonly)，在运行时会报错。解决方法如下： cd /home/WeekSummaryAppchmod 777 db.sqlite3cd ..chmod 777 * 改变它的权限后就能正常使用了。]]></content>
      <tags>
        <tag>django</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[win10+deepin双系统安装(windows引导)]]></title>
    <url>%2F2019%2F10%2F20%2Fwin10-deepin%E5%8F%8C%E7%B3%BB%E7%BB%9F%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[对deepin的简单介绍Deepin操作系统是由武汉深之度科技有限公司基于Debian开发的Linux发行版(大家熟知的Ubuntu其实也是基于Debian开发的呢)。deepin操作系统是一个基于 Linux 的操作系统，专注于使用者对日常办公、学习、生活和娱乐的操作体验的极致，适合笔记本、桌面计算机和一体机。它包含了所有您需要的应用程序，网页浏览器、幻灯片演示、文档编辑、电子表格、娱乐、声音和图片处理软件，即时通讯软件等等。个人认为非常适合国人使用。 什么是双系统简单来讲就是一个电脑包含两个操作系统。双系统在安装的时候，两个系统是分别装在不同的分区内，后安装的系统不会覆盖前一个系统。而且每个单独的系统都有自己的分区格式，不会造成冲突的。安装了双系统后，在启动的时候，有一个多重启动的选择菜单，可以选择进入那个操作系统。当前状态下，只有一个系统是在运行的，不能随意的切换。如果想要进入另外一个，就要重新启动，重新选择。 为什么我选择双系统而不是虚拟机因为自己最近在学习linux相关操作，入手了被捧为圣经的’鸟哥的linux私房菜 基础篇’一书，由他在书中的建议故选择了双系统。书中tips如下： Tips:一般来说，你还可以在Windows操作系统上面安装Virtualbox之类的软件，让你可以在Windows系统上面『同时』使用Linux系统， 就是两个操作系统同时启动！不过，那样的环境比较复杂，尤其Virtualbox环境中很多硬件都是模拟的， 会让新手很难理解系统控制原理。基本上，鸟哥很不建议您使用这样的方式来学习Linux喔！ 装机时您所需要了解的知识：挂载点 所谓的『挂载』就是利用一个目录当成进入点，将磁盘分区槽的数据放置在该目录下； 也就是说，进入该目录就可以读取该分割槽的意思。这个动作我们称为『挂载』，那个进入点的目录我们称为『挂载点』。 由於整个Linux系统最重要的是根目录，因此根目录一定需要挂载到某个分割槽的。 至於其他的目录则可依使用者自己的需求来给予挂载到不同的分割槽。 更详细的原书,ctrl+f搜索挂载即可。 UEFI全称“统一的可扩展固件接口”(Unified Extensible Firmware Interface)， 是一种详细描述类型接口的标准。是一种全新的启动方式，一般较新的电脑都带这种启动方式，通常出厂预装Win10的系统是默认以这种方式启动的（有些USB启动盘启动PE后，无法发现硬盘，通常是由于USB启动盘未设置UEFI启动方式）。 GPT分区全名为Globally Unique Identifier Partition Table Format，即全局唯一标示磁盘分区表格式。GPT还有另一个名字叫做GUID分区表格式，我们在许多磁盘管理软件中能够看到这个名字。而GPT也是UEFI所使用的磁盘分区格式。 制作启动盘 U盘制作启动盘最好选择8G以上的(不然可能连系统镜像都装不下就尴尬了),接着去deepin官网将deepin的ios镜像给下载下来(其他linux发行版也是如此)，接着你需要一个软件来将你的ios文件解压到U盘里边。 这里我推荐一款rufus,它可以让你在制作启动盘时选择磁盘分区表类型与不同引导方式。 使用该软件制作启动盘时有三个地方需要关注。 引导类型选择 就是选择我们刚刚下载好的Linux镜像文件，待会进行解压 分区类型 指磁盘分区表类型(GPT与MRB) 目标系统类型 指引导电脑开机的模式(UEFI与Legacy)后面两个与电脑而异，大家应小心选择，不然可能连启动盘都找不着。 这里系统的磁盘分区表主要类型有两种，分别为GUID(也叫 GPT)与 MRB，大致有两种方法获知自己电脑的磁盘分区表类型。 桌面–&gt;计算机图标–&gt;右键–&gt;管理–&gt;磁盘管理–&gt;选择一个磁盘（注意是磁盘不是分区）–&gt;右键–&gt;属性–&gt;鼠标左键点击“卷”–&gt;查看“磁盘分区形式” win + R 打开运行框 输入CMD回车 在CMD里输入diskpart 回车等显示出 DISKPART&gt; 输入list disk 回车 GPT下带 * 号的就是GPT分区表的硬盘(带有 * 号的即为UEFI方式启动，没有则为传统的Legacy方式启动) 目标系统类型主要有(UEFI与Legacy),获取方式如上。 UEFI和Legacy是两种不同的引导方式,UEFI是新式的BIOS，Legacy是传统BIOS。你在UEFI模式下安装的系统，只能用UEFI模式引导；同理，如果你是在Legacy模式下安装的系统，也只能在legacy模式下进系统。uefi只支持64为系统且磁盘分区必须为gpt模式，传统BIOS使用Int 13中断读取磁盘，每次只能读64KB，非常低效，而UEFI每次可以读1MB，载入更快。 完成一些上边的内容后，点击开始，耐心等待即可。 磁盘分区这里可以理解为在你的硬盘上划一块区域出来给linux，操作如下：进入 桌面–&gt;计算机图标–&gt;右键–&gt;管理–&gt;磁盘管理这里找一个空间剩余比较多的硬盘(我的1T机械硬盘),压缩20G出来给Linux(最好大于20G)，确定后就不用再管了(即让这个磁盘处于未分配状态即可) BOIS里的操作各品牌电脑进入快捷键见如下文章 进入Boot，将U盘设为首选启动项，保存并重启。 重启后选择INSTALL deepin,然后进行配置选项(安装语言，用户创建，时区选择，安装位置)，这里的安装位置应选则我们刚才在磁盘分区里分出的那个区域。稍作等待就能完成安装。 遇到问题 在开机后卡在了logo界面,解决办法 连不了网，重启一遍完事。 这样Deepin就安装好了，您可以在BIOS里设置首选启动项，接下来开启Linux之旅吧！]]></content>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[（python爬虫练手）爬取百度音乐--翻车实例（requests+re）]]></title>
    <url>%2F2019%2F09%2F19%2F%EF%BC%88python%E7%88%AC%E8%99%AB%E7%BB%83%E6%89%8B%EF%BC%89%E7%88%AC%E5%8F%96%E7%99%BE%E5%BA%A6%E9%9F%B3%E4%B9%90-%E7%BF%BB%E8%BD%A6%E5%AE%9E%E4%BE%8B%EF%BC%88requests-re%EF%BC%89%2F</url>
    <content type="text"><![CDATA[文章简介这篇文章是来爬取百度音乐的歌曲的，本来看半年前的视频时是可以爬取vip音乐的接口的，但是现在却发现这个vip音乐的接口里边全是错误。心里那个不爽哟。但这也从另一个角度告诉我们技术要时常更新，不然就out了。。。。 爬取一首歌只针对一首歌可以模仿梨视频的操作 123url = &apos;http://audio02.dmhmusic.com/71_53_T10045991216_128_4_1_0_sdk-cpm/cn/0207/M00/7B/92/ChR461z86tSAKx30AEiissuSids044.mp3?xcode=079bc3704de9af1351a5828c0637a6378f5fe73&apos;response = requests.get(url) 这两行代码即获取这个音乐的真实地址的源代码。 12with open(&apos;小半.mp3&apos;,&apos;wb&apos;) as f: f.write(response.content) 这两行代码即以二进制保存这个音乐。感jo还行，接下来我们来爬取多个音乐。至此，一首歌爬取完毕。 爬取多首歌这里与爬取梨视频的操作有些不一样，我们回顾一波梨视频的做法，我们找到这个视频的地址，接着我们在源代码里找到了这个地址，然后以这个为基础进行一系列操作。但是这个操作再这个程序里行不通。所以另找方向。不卖关子了，这里随便找两音乐，对比一波他们真实地址的异同这里我随意找了两个 123http://audio02.dmhmusic.com/71_53_T10046145023_128_4_1_0_sdk-cpm/cn/0208/M00/67/1B/ChR461shSbeAdAWeAFB5VDzrIFc155.mp3?xcode=49e1b2339ff3225a51b5df12a0b174c0bef9e61http://audio02.dmhmusic.com/71_53_T10045991216_128_4_1_0_sdk-cpm/cn/0207/M00/7B/92/ChR461z86tSAKx30AEiissuSids044.mp3?xcode=079bc3704de9af1351a5828c0637a6378f5fe73 发现他们后边的xcode参数比较唯一，于是我们在网页里打开开发者工具并进入到network属性里边，发者xcode参数值并control f寻找这段参数。发现找到的内容里有一个的请求头是一个api我们进入这个接口发现里边全是json数据，但是这个时候去print是无法打印出来的，因为里边有一段多的代码，这段代码并不是json数据。不要慌，这个时候一定会有种感jo，这个api太长了，我们能不能删掉一些参数，让它变短，可以，最终的结果为http://musicapi.taihe.com/v1/restserver/ting?method=baidu.ting.song.playAAC&amp;songid=xxxxxx,而且那一段多余的代码也被删除了，简直爽到飞起。这里建议导入一个模块import pprint用它来输出json数据会比较好看.欲知为何飞起，请看下边代码。 123url = &apos;http://music.taihe.com/artist/7994&apos;response = requests.get(url)list = re.findall(&apos;class=&quot;songname&quot;.*?&lt;a href=&quot;/song/(.*?)&quot;&apos;,response.text, re.S) 这里相当于爬取主页面的歌曲的singid，为后边访问接口做准备. 1234567891011121314for l in list: url_api = &apos;http://musicapi.taihe.com/v1/restserver/ting?method=baidu.ting.song.playAAC&amp;songid=&apos; + l resp = requests.get(url_api) json = resp.json() # pprint.pprint(json) if &apos;bitrate&apos; in json.keys(): music = json[&apos;bitrate&apos;][&apos;file_link&apos;] music_name = json[&apos;songinfo&apos;][&apos;title&apos;] with open(music_name+&quot;.aac&quot;,&apos;wb&apos;) as f: f.write(requests.get(music).content) print(music_name) print(music,music_name ) else: continue 我们可以使用pprint。pprint(json)看一波json数据发现这里的链接，就是这个歌曲的真是链接。ok，接下来咱就爬就完事了。 123url_api = &apos;http://musicapi.taihe.com/v1/restserver/ting?method=baidu.ting.song.playAAC&amp;songid=&apos; + lresp = requests.get(url_api)json = resp.json() 这里是获取api的json数据。 12345music = json[&apos;bitrate&apos;][&apos;file_link&apos;] music_name = json[&apos;songinfo&apos;][&apos;title&apos;] with open(music_name+&quot;.aac&quot;,&apos;wb&apos;) as f: f.write(requests.get(music).content) print(music_name) 这里是获取json数据里的歌曲真是的值的链接和歌曲的名字，并将它保存下来，print(music_name)是为了在程序运行时好歹看到一些东西，不然会有些难过。但是我这么干了之后，发现爬了一会就报错了，说是找不到啥啥啥，所以我加入了if语句来判断这个键在不在这个字典里。(json数据挺像python的字典的，特意print(type(json),打印的是dict)。但是尴尬的事情来了，因为我爬的是周董的歌(众所周知，周董的歌大都要vip)，所以我这些vip的内容我全没爬到，桑心，暂时没解决。]]></content>
      <tags>
        <tag>Python Spider</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[（python爬虫练手）爬取梨视频（requests+re）]]></title>
    <url>%2F2019%2F09%2F19%2F%EF%BC%88python%E7%88%AC%E8%99%AB%E7%BB%83%E6%89%8B%EF%BC%89%E7%88%AC%E5%8F%96%E6%A2%A8%E8%A7%86%E9%A2%91%EF%BC%88requests-re%EF%BC%89%2F</url>
    <content type="text"><![CDATA[文章简介这篇文章主要是对梨视频的爬取。 爬取单个视频点开梨视频的某一个视频，发现是可以播放的，但是它并没有提供下载的按钮。打开查看网页的源代码，点击network属性，筛选media内容(这里可能需要先点击一下视频，及让其播放，这样就可以从服务器端得到它传回来的视频的真实地址)访问这个地址就可以看到这个视频，接着我们就开始对这单个视频进行爬取。 12with open(&apos;0.0.mp4&apos;,&apos;wb&apos;) as f: f.write(response.content) 由于视频，图像，音乐均为二进制存储，而response.content将返回一个二进制源代码。open(&#39;0.0.mp4&#39;,&#39;wb&#39;)这个是创建一个名为0.0.MP4的文件(若已经有这个文件存在则重写这个文件),wb表示以二进制形式写入。至此，单个视频爬取完毕。 爬取多个视频这里回顾一下单个视频爬取方法，先认为的打开一个视频，然后再开用发者工具找到从服务端返回的视频地址，接着进行爬取。但爬取多个视频就不能这么认为操作了，毕竟计算机可不知道怎么搞。这里是无意中在梨视频网页的源代码中找到了这个视频的真实地址(这里只是巧合，不可能所有的视频都这么干，但在这里这条路竟然走得通，那咱就该坚定不移的走下去，其实就是我现在不会别的操作)。ok，我们再来捋一捋爬取多个视频的思路，先从一个类似小说目录的网页(可以看到多个视频)找到每一个视频链接，从这个链接的源代码里找到这个视频的真实地址，再进行下载，奈斯，接下来咱们开始。 12response = requests.get(&apos;https://www.pearvideo.com/category_8&apos;)url_list = re.findall(&apos;class=&quot;vervideo-bd&quot;.*?href=&quot;(.*?)&quot;&apos;, response.text,re.S) 这个是请求得到视频首页的响应体并用正则表达式找到网页链接。这里为啥要用正则？因为这里的链接存在与一串字符串里而不再标签里，所以用其他的解析库pyquery，xpath等没办法得到。findall()方法将返回一个列表，列表里的内容是每一个满足条件的类容，即用（）括起来的，re.S即可匹配多行 12345678for u in url_list: url = &apos;https://www.pearvideo.com/&apos; + u resp = requests.get(url) name = re.findall(&apos;&lt;h1 class=&quot;video-tt&quot;&gt;(.*?)&lt;/h1&gt;&apos;,resp.text,re.S)[0] video_url = re.findall(&apos;srcUrl=&quot;(.*?)&quot;&apos;,resp.text,re.S)[0] video_content = requests.get(video_url).content with open(name+&quot;.mp4&quot;,&apos;wb&apos;) as f: f.write(video_content) 这里的操作就是得到该视频的页面，并从这个页面里找到这个视频的名字，整整存储视频的地址，接着下载就完事了。至此，多个视频爬取完毕。]]></content>
      <tags>
        <tag>Python Spider</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[（python爬虫练手）爬取剑来小说（requests+pyquery）]]></title>
    <url>%2F2019%2F09%2F19%2F%EF%BC%88python%E7%88%AC%E8%99%AB%E7%BB%83%E6%89%8B%EF%BC%89%E7%88%AC%E5%8F%96%E5%89%91%E6%9D%A5%E5%B0%8F%E8%AF%B4%EF%BC%88requests-pyquery%EF%BC%89%2F</url>
    <content type="text"><![CDATA[文章简介这篇文章主要爬取剑来小说,大致的原理方法跟上篇爬取读者文摘的文章类似。 爬取小说 12import requestsfrom pyquery import PyQuery as pq 这里我导入了requests库(网页请求)和pyquery库(解析)。为啥没有用BeautifulSoup？下边是一个博主对python四大主流解析库的性能测试(原文地址)可以发现，beautifulsoup非常的慢，所以建议多学几个解析库，以备不时之需。 123url = &apos;http://www.jianlaixiaoshuo.com/&apos;response = requests.get(url)response.encoding = response.apparent_encoding 这三行分是获取这个url的网页源代码，并将它进行一个转码。 12html = pq(response.text)titles = html(&apos;dl &gt; dd a&apos;) 这两行就是pyquery的语法了，html = pq(response.text)即解析这个网页的源代码，titles = html(&#39;dl &gt; dd a&#39;)是来筛选每一个文章的地址(其实获取的是一个a标签，每个文章的地址将在遍历时得到)，这里返回的是类型为&lt;class &#39;pyquery.pyquery.PyQuery&#39;&gt;的许多对象(可以使用print(type(titles)查看)，所以我们在下边遍历时要使用items()，即取出每一个对象。 1234567891011for title in titles.items(): newurl = &apos;http://www.jianlaixiaoshuo.com/&apos; + title.attr.href newresponse = requests.get(newurl) newresponse.encoding = newresponse.apparent_encoding newhtml = pq(newresponse.text) newtitle = newhtml(&apos;#BookCon &gt; h1&apos;).text() newtext = newhtml(&apos;#BookText &gt; p&apos;).text() print(newtitle) if newtext == &apos;&apos;: newtext = newhtml(&apos;#readerFs &gt; p&apos;).text() print(newtext) 这里的title.attr.href是获取title里的href属性值，构成一个新的url，这个即每章小说的地址。接下来的操作就是在单篇文章里获取章节名与文章内容了，不再多述。关于最后的if语句，在爬取时我发现有些章节内容为空，于是我进去查看，发现我筛选的内容里(newtext)居然没啥东西，而真正的文章却出现在了id=&quot;readerFs&quot;的标签下，故有此判断。 没有解决的问题爬取小说的时候我发现有些章节出现了乱码的情况，但是代码应该没问题，暂未解决此问题。 pyquery的一些用法介绍要说的是pyquery的一些用法，筛选时.表示class属性，而#则表示的是id属性(newhtml(&#39;#BookText &gt; p&#39;))，a &gt; b表示a标签下所有标签为b的子节点，而a b表示a标签下所有标签为b的子孙节点。text()方法是获取这个标签里边的文本(如果print(type(newtext))就会发现使用text()方法得到的类型为str).]]></content>
      <tags>
        <tag>Python Spider</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[（python爬虫练手）爬取读者网站（requests+beautifulsoup]]></title>
    <url>%2F2019%2F09%2F17%2F%EF%BC%88python%E7%88%AC%E8%99%AB%E7%BB%83%E6%89%8B%EF%BC%89%E7%88%AC%E5%8F%96%E8%AF%BB%E8%80%85%E7%BD%91%E7%AB%99%EF%BC%88requests-beautifulsoup%EF%BC%89%2F</url>
    <content type="text"><![CDATA[文章简介这篇文章主要是对python爬虫项目的一个小练习，会使用到的python第三方库为requests与BeautifulSoup。爬取的对象为读者文摘部分内容。 爬取一篇文章我先爬取一篇文章，为后边爬取多篇文章打基础。这里我采用的方式是先把代码贴出来，再做解读。 12import requestsfrom bs4 import BeautifulSoup 这里是导入requests库与BeautifulSoup库。 12url = &apos;http://www.rensheng5.com/duzhewenzhai/rensheng/id-176946.html&apos;response = requests.get(url) url即我们要爬取文章的网址，第二行代码response = requests.get(url)调用了requests里的get方法将url作为参数传进，它将返回一个网页的响应体，这个响应体即包含了我们接下来要操作的内容。(这里可以做一个小实验，比如直接print(response)，这时打印出来的会是&lt;Response [200]&gt;，这个200我理解为网页的状态码，即连通了网站) 1html = response.content.decode(&apos;gbk&apos;) 这里我们也可以做一个尝试，比如在这行代码之前print(response.content)或者print(response.text),前者会返回一个网页源代码的二进制形式而后者会返回一个带缩进的网页源代码。那接着咱们再回到前边这行代码，这行代码主要是改变源代码的编码格式为gbk，因为不这么干的话，汉字有可能出现乱码。 1soup = BeautifulSoup(html,&apos;lxml&apos;) 这里我调用了BeautifulSoup，第一个参数为指定编码格式的源代码，而第二个lxml是一个html的解析器(这玩意要自己安装pip install lxml,如果比较懒，可以使用自带的解析器html.parse),说了这么多，这玩意有啥用呢？把它打印一下就可以明显看出端倪print(soup),这里打印出的网页源代码的缩进没有了，目的是为了后边更好的筛选所需内容。 12print(soup.find(&apos;div&apos;,class_=&quot;artview&quot;).find(&apos;h1&apos;).get_text())print(soup.find(&apos;div&apos;,class_=&quot;artbody&quot;).find(&apos;p&apos;).get_text()) 这两行是我要找的内容，find()方法是找到第一个满足条件的式子，与之对应的是find_all()方法，它会匹配所有满足条件的式子。它们里边的参数大致相同，第一个参数为标签名，第二个是它所对应的的属性,由于class是python的关键字，所以在这后边加一下划线来替代。如果做到了这一步不调用后边的get_text()方法，那么我们最终打印的会是一个带标签的式子，而这里的get_text()方法就是获取标签里边文本的操作。致辞，单个文章爬取完成。 爬取多篇文章这里我来先捋捋思路，我爬取一篇文章的时候是先有了这篇文章的网址，接着爬取。而对文章而言一般都会有个类似目录的玩意，这上边会有各个文章连链接，简单查看就能知道他们的链接在a标签的href属性里边，也就是说我们只要从总目录里先获取每一个文章的链接，接着进行访问，重复爬取一篇文章的思路就行了。奈斯，我们开始动手吧。 12345def getSoup(url): response = requests.get(url) html = response.content.decode(&apos;gbk&apos;) soup = BeautifulSoup(html,&apos;lxml&apos;) return soup 这里我定义了一个函数getSoup(url),它实现的功能是获取传入url的源代码并对其进行解析，最后把解析好的源代码返回。 123456789def main(url): soup = getSoup(url) lis = soup.find(&apos;ul&apos;,class_=&quot;p7&quot;).find_all(&apos;li&apos;) for li in lis: href = li.find(&apos;a&apos;)[&apos;href&apos;] text_soup = getSoup(href) title = text_soup.find(&apos;div&apos;,class_=&quot;artview&quot;).find(&apos;h1&apos;).get_text() text = text_soup.find(&apos;div&apos;,class_=&quot;artbody&quot;).find(&apos;p&apos;).get_text() print(href + &apos;\n&apos; + title + &apos;\n&apos; + text) 这个主函数实现的内容也比较清楚，首先将传入的url进行解析(这里的url是目录的网址，即包含所有文章链接的网址)，接着我根据网页的源代码筛选出我需要的标签[得到的是一个列表]，根据我的需要获取href属性的值(这里的值就是每一篇文章的链接)，接着我把每一篇文章的链接传给getSoup()函数返回解析后的(文章)源代码，接下来的操作就是对单篇文章进行爬取了，最后进行打印。 123if __name__ == &apos;__main__&apos;: url = &apos;http://www.rensheng5.com/duzhewenzhai/&apos; main(url) 这里主要是传入目录的url，并调用main函数 至此，多篇文章爬取完成。具体的网站内容可以访问我所使用的url，单篇文章：http://www.rensheng5.com/duzhewenzhai/rensheng/id-176946.html.多篇文章：http://www.rensheng5.com/duzhewenzhai/.]]></content>
      <tags>
        <tag>Python Spider</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用hexo+nexT+github搭建个人博客]]></title>
    <url>%2F2019%2F08%2F04%2F%E4%BD%BF%E7%94%A8hexo-nexT-github%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%2F</url>
    <content type="text"><![CDATA[软件安装1.node.js2.git3.hexo 安装好前两个后任意地方鼠标右键打开Git Bash，输入$ npm install hexo-cli -g来安装hexo，但由于国内网络原因我们可能安装不上，这里使用淘宝的npm镜像$ npm install -g cnpm --registry=https://registry.npm.taobao.org，再用它在安装hexo $ cnpm install -g hexo-cli。(以上操作均在Git Bash完成) 使用到的官方文档1.hexo2.next 常用到的命令 hexo g g为generate的缩写，意为生成静态文件 hexo s s为server的缩写，意为启动服务 hexo clean 官方解释为清除缓存文件 (db.json) 和已生成的静态文件 (public)。在某些情况（尤其是更换主题后），如果发现您对站点的更改无论如何也不生效，您可能需要运行该命令。 hexo d d为deploy部署网站之意，即将本地的博客部署到仓库，在后边会用到再补充。初始化博客任意位置新建并进入一个文件夹，右键打开Git Bashhexo init初始化博客下载next主题在你博客的根目录下打开Git Bash,输入git clone https://github.com/iissnan/hexo-theme-next themes/next。耐心等待即可。对主题的配置 使用next主题，并将语言改为中文进入博客的根文件，打 开_config.yml 文件，找到Site里的language属性，在后面加上zh-Hans(简体中文)这里要注意langeuage：后面要有一个空格。接着一直滑到底部，看到Extensions属性里有一个theme，将后面的landscape更改为next，着用用Git Bash点击自己的博客文件，输入hexo g，再输入hexo s进入网页即可观看效果。 增加并配置标签页与分类页首先在博客的根目录里边进入theme/next，找到_config.yml,打开后找到menu这里会发现tags跟categories被注释了，将注释删去后运行，我们会得到这样的页面随后点击标签，发现页面错误，我们接下来配置这个页面。我们先进入source文件夹里，只有一个_posts,接着我们在Git Bash里输入hexo n page tags会发source这个文件夹里会多一个tags文件夹，进入里边的index修改如下这里多加入了一个type ，加于不加效果如下:(先是加的，后是不加)当然如果我加入tpye但是后边的值是categories效果如下接下来分类页面也是如此。 更改主题在theme/next下找到_config.yml找到scheme，这里有提供四个主题，可以根据喜欢来更改。这里我选的是最后一个。效果如下： 配置个人信息(更改自己的头像，姓名与title)打开博客下的配置文件_config.yml,分别修改title与author，再搜索avatar，可以使用图片的url，或者是图片的路径(图片要在source里) 配置交友链接打开next里的配置文件，找到social，分别启用Github，E-mail，也可以自己加上微博或者其他的，配置与效果如下。(这里我加了自己的网易云音乐) 让自己的头像变为圆形，添加旋转功能进入themes\next\source\css_common\components\sidebar/sidebar-author.styl修改如下：第一个是让头像变为圆形，注释的是让头像旋转。 设置阅读全文这里有两种方法一种是修改主题里的配置文件另一个是在文章里加入&lt;!-- more --&gt;代码 添加动态背景同样也是在主题的配置文件里搜索canvas，会有四个动态背景供选择，我选择none。 添加建站时间在主题的配置文件里加入since字段eg:since:2010-2011这个时间会在页面最下面显示。 设置链接颜色在\themes\next\source\css_common\components\post\post.styl里设置如下可以设置自己喜欢的颜色(十六进制颜色表) 增加搜索服务进入next官方文档添加一个local search的搜索服务(也可以用别的，但是别的要注册啥的麻烦，所以我就用了它) 添加评论系统进入第三方服务的官方文档可以使用来必力的评论系统，但因为国内网络原因，加载非常慢被广大网友吐槽。我本来想用valine的评论系统，但注册后发现需要实名认证，果断放弃。所以我的博客没有评论系统。 增加统计服务在主题文件配置如下(这里用的是不蒜子统计)虽然我觉得有些不对劲，但还是先用着吧！用了会发现不蒜子无法正常计数(访问几次没啥效果)，发现官方给的说法是解决方法：进入\themes\next\layout_third-party\analytics\busuanzi-counter.swig将&lt;script async src=&quot;https://busuanzi/busuanzi/2.3/busuanzi.pure.mini.js&quot;&gt;&lt;/script&gt;改为&lt;script async src=&quot;https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js&quot;&gt;&lt;/script&gt;即可 设置加载效果在主题的配置文件搜索pace字段配置如下有多种选择可以尝试。 字数统计与阅读时长在博客根目录输入如下命令$ npm install hexo-wordcount --save然后在主题的配置文件搜索post_wordcount关键字，修改如下 至此本地博客主题配置完成，下面将它部署到Github即可。 将本地博客部署到Github 新建仓库注册/登录Github，新建仓库(这里要注意仓库名的形式一定要为yourname.github.io，yourname是你的Github的名字) 接着我们来为Git配置Github的账号邮箱以及密钥$ git config --global user.name &quot;flyingdigital&quot;// 你的github用户名，非昵称$ git config --global user.email &quot;zhaoqian123@outlook.com&quot;// 填写你的github注册邮箱(输入完这两个后是啥都没有的)接着我们在电脑上生成密钥并将它添加到Github上边，$ ssh-keygen -t rsa -C &quot;你的github注册邮件地址&quot;用这个来实现本地与远程仓库的连接。第一次的时候密钥会直接显示，复制下来即可。如果已经生成过，可以按照它的提示在自己电脑上找到同样，这里也是将它复制下来。在自己Github的settings里找到SSh and GPG keys项，在SSH keys里加入刚刚复制的密钥(title可以随意起一个)在这里可以先复制好自己的仓库的地址 在hexo里配置进入hexo的配置文件，找到depoly字段，配置如下 远程部署在博客根目录打开Git Bash输入hexo g,hexo d这里第二个命令可能会出现问题，大概就是说找不到git，这里我们安装一下插件就可以解决$ npm install hexo-deployer-git --save,安装插件后在输入hexo d即可部署完成。上传文章在博客的根目录里打开GitBash，输入hexo n 你文章的名字即可创建新的文章，这个在source/_posts里可以看到 至此，博客搭建完毕。主题除了next也可以使用其他的，也可在hexo的官方文档里的主题里选择。]]></content>
      <categories>
        <category>总结</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
</search>
