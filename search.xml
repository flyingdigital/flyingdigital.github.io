<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[（python爬虫练手）爬取读者网站（requests+beautifulsoup）]]></title>
    <url>%2F2019%2F09%2F17%2F%EF%BC%88python%E7%88%AC%E8%99%AB%E7%BB%83%E6%89%8B%EF%BC%89%E7%88%AC%E5%8F%96%E8%AF%BB%E8%80%85%E7%BD%91%E7%AB%99%EF%BC%88requests-beautifulsoup%EF%BC%89%2F</url>
    <content type="text"><![CDATA[文章简介这篇文章主要是对python爬虫项目的一个小练习，会使用到的python第三方库为requests与BeautifulSoup。爬取的对象为读者文摘部分内容。 爬取一篇文章我先爬取一篇文章，为后边爬取多篇文章打基础。这里我采用的方式是先把代码贴出来，再做解读。 12import requestsfrom bs4 import BeautifulSoup 这里是导入requests库与BeautifulSoup库。 12url = &apos;http://www.rensheng5.com/duzhewenzhai/rensheng/id-176946.html&apos;response = requests.get(url) url即我们要爬取文章的网址，第二行代码response = requests.get(url)调用了requests里的get方法将url作为参数传进，它将返回一个网页的响应体，这个响应体即包含了我们接下来要操作的内容。(这里可以做一个小实验，比如直接print(response)，这时打印出来的会是&lt;Response [200]&gt;，这个200我理解为网页的状态码，即连通了网站) 1html = response.content.decode(&apos;gbk&apos;) 这里我们也可以做一个尝试，比如在这行代码之前print(response.content)或者print(response.text),前者会返回一个网页源代码的二进制形式而后者会返回一个带缩进的网页源代码。那接着咱们再回到前边这行代码，这行代码主要是改变源代码的编码格式为gbk，因为不这么干的话，汉字有可能出现乱码。 1soup = BeautifulSoup(html,&apos;lxml&apos;) 这里我调用了BeautifulSoup，第一个参数为指定编码格式的源代码，而第二个lxml是一个html的解析器(这玩意要自己安装pip install lxml,如果比较懒，可以使用自带的解析器html.parse),说了这么多，这玩意有啥用呢？把它打印一下就可以明显看出端倪print(soup),这里打印出的网页源代码的缩进没有了，目的是为了后边更好的筛选所需内容。 12print(soup.find(&apos;div&apos;,class_=&quot;artview&quot;).find(&apos;h1&apos;).get_text())print(soup.find(&apos;div&apos;,class_=&quot;artbody&quot;).find(&apos;p&apos;).get_text()) 这两行是我要找的内容，find()方法是找到第一个满足条件的式子，与之对应的是find_all()方法，它会匹配所有满足条件的式子。它们里边的参数大致相同，第一个参数为标签名，第二个是它所对应的的属性,由于class是python的关键字，所以在这后边加一下划线来替代。如果做到了这一步不调用后边的get_text()方法，那么我们最终打印的会是一个带标签的式子，而这里的get_text()方法就是获取标签里边文本的操作。致辞，单个文章爬取完成。 爬取多篇文章这里我来先捋捋思路，我爬取一篇文章的时候是先有了这篇文章的网址，接着爬取。而对文章而言一般都会有个类似目录的玩意，这上边会有各个文章连链接，简单查看就能知道他们的链接在a标签的href属性里边，也就是说我们只要从总目录里先获取每一个文章的链接，接着进行访问，重复爬取一篇文章的思路就行了。奈斯，我们开始动手吧。 12345def getSoup(url): response = requests.get(url) html = response.content.decode(&apos;gbk&apos;) soup = BeautifulSoup(html,&apos;lxml&apos;) return soup 这里我定义了一个函数getSoup(url),它实现的功能是获取传入url的源代码并对其进行解析，最后把解析好的源代码返回。 123456789def main(url): soup = getSoup(url) lis = soup.find(&apos;ul&apos;,class_=&quot;p7&quot;).find_all(&apos;li&apos;) for li in lis: href = li.find(&apos;a&apos;)[&apos;href&apos;] text_soup = getSoup(href) title = text_soup.find(&apos;div&apos;,class_=&quot;artview&quot;).find(&apos;h1&apos;).get_text() text = text_soup.find(&apos;div&apos;,class_=&quot;artbody&quot;).find(&apos;p&apos;).get_text() print(href + &apos;\n&apos; + title + &apos;\n&apos; + text) 这个主函数实现的内容也比较清楚，首先将传入的url进行解析(这里的url是目录的网址，即包含所有文章链接的网址)，接着我根据网页的源代码筛选出我需要的标签[得到的是一个列表]，根据我的需要获取href属性的值(这里的值就是每一篇文章的链接)，接着我把每一篇文章的链接传给getSoup()函数返回解析后的(文章)源代码，接下来的操作就是对单篇文章进行爬取了，最后进行打印。 123if __name__ == &apos;__main__&apos;: url = &apos;http://www.rensheng5.com/duzhewenzhai/&apos; main(url) 这里主要是传入目录的url，并调用main函数 至此，多篇文章爬取完成。具体的网站内容可以访问我所使用的url，单篇文章：http://www.rensheng5.com/duzhewenzhai/rensheng/id-176946.html.多篇文章：http://www.rensheng5.com/duzhewenzhai/.]]></content>
      <tags>
        <tag>Python Spider</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用hexo+nexT+github搭建个人博客]]></title>
    <url>%2F2019%2F08%2F04%2F%E4%BD%BF%E7%94%A8hexo-nexT-github%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%2F</url>
    <content type="text"><![CDATA[软件安装1.node.js2.git3.hexo 安装好前两个后任意地方鼠标右键打开Git Bash，输入$ npm install hexo-cli -g来安装hexo，但由于国内网络原因我们可能安装不上，这里使用淘宝的npm镜像$ npm install -g cnpm --registry=https://registry.npm.taobao.org，再用它在安装hexo $ cnpm install -g hexo-cli。(以上操作均在Git Bash完成) 使用到的官方文档1.hexo2.next 常用到的命令 hexo g g为generate的缩写，意为生成静态文件 hexo s s为server的缩写，意为启动服务 hexo clean 官方解释为清除缓存文件 (db.json) 和已生成的静态文件 (public)。在某些情况（尤其是更换主题后），如果发现您对站点的更改无论如何也不生效，您可能需要运行该命令。 hexo d d为deploy部署网站之意，即将本地的博客部署到仓库，在后边会用到再补充。初始化博客任意位置新建并进入一个文件夹，右键打开Git Bashhexo init初始化博客下载next主题在你博客的根目录下打开Git Bash,输入git clone https://github.com/iissnan/hexo-theme-next themes/next。耐心等待即可。对主题的配置 使用next主题，并将语言改为中文进入博客的根文件，打 开_config.yml 文件，找到Site里的language属性，在后面加上zh-Hans(简体中文)这里要注意langeuage：后面要有一个空格。接着一直滑到底部，看到Extensions属性里有一个theme，将后面的landscape更改为next，着用用Git Bash点击自己的博客文件，输入hexo g，再输入hexo s进入网页即可观看效果。 增加并配置标签页与分类页首先在博客的根目录里边进入theme/next，找到_config.yml,打开后找到menu这里会发现tags跟categories被注释了，将注释删去后运行，我们会得到这样的页面随后点击标签，发现页面错误，我们接下来配置这个页面。我们先进入source文件夹里，只有一个_posts,接着我们在Git Bash里输入hexo n page tags会发source这个文件夹里会多一个tags文件夹，进入里边的index修改如下这里多加入了一个type ，加于不加效果如下:(先是加的，后是不加)当然如果我加入tpye但是后边的值是categories效果如下接下来分类页面也是如此。 更改主题在theme/next下找到_config.yml找到scheme，这里有提供四个主题，可以根据喜欢来更改。这里我选的是最后一个。效果如下： 配置个人信息(更改自己的头像，姓名与title)打开博客下的配置文件_config.yml,分别修改title与author，再搜索avatar，可以使用图片的url，或者是图片的路径(图片要在source里) 配置交友链接打开next里的配置文件，找到social，分别启用Github，E-mail，也可以自己加上微博或者其他的，配置与效果如下。(这里我加了自己的网易云音乐) 让自己的头像变为圆形，添加旋转功能进入themes\next\source\css_common\components\sidebar/sidebar-author.styl修改如下：第一个是让头像变为圆形，注释的是让头像旋转。 设置阅读全文这里有两种方法一种是修改主题里的配置文件另一个是在文章里加入&lt;!-- more --&gt;代码 添加动态背景同样也是在主题的配置文件里搜索canvas，会有四个动态背景供选择，我选择none。 添加建站时间在主题的配置文件里加入since字段eg:since:2010-2011这个时间会在页面最下面显示。 设置链接颜色在\themes\next\source\css_common\components\post\post.styl里设置如下可以设置自己喜欢的颜色(十六进制颜色表) 增加搜索服务进入next官方文档添加一个local search的搜索服务(也可以用别的，但是别的要注册啥的麻烦，所以我就用了它) 添加评论系统进入第三方服务的官方文档可以使用来必力的评论系统，但因为国内网络原因，加载非常慢被广大网友吐槽。我本来想用valine的评论系统，但注册后发现需要实名认证，果断放弃。所以我的博客没有评论系统。 增加统计服务在主题文件配置如下(这里用的是不蒜子统计)虽然我觉得有些不对劲，但还是先用着吧！用了会发现不蒜子无法正常计数(访问几次没啥效果)，发现官方给的说法是解决方法：进入\themes\next\layout_third-party\analytics\busuanzi-counter.swig将&lt;script async src=&quot;https://busuanzi/busuanzi/2.3/busuanzi.pure.mini.js&quot;&gt;&lt;/script&gt;改为&lt;script async src=&quot;https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js&quot;&gt;&lt;/script&gt;即可 设置加载效果在主题的配置文件搜索pace字段配置如下有多种选择可以尝试。 字数统计与阅读时长在博客根目录输入如下命令$ npm install hexo-wordcount --save然后在主题的配置文件搜索post_wordcount关键字，修改如下 至此本地博客主题配置完成，下面将它部署到Github即可。 将本地博客部署到Github 新建仓库注册/登录Github，新建仓库(这里要注意仓库名的形式一定要为yourname.github.io，yourname是你的Github的名字) 接着我们来为Git配置Github的账号邮箱以及密钥$ git config --global user.name &quot;flyingdigital&quot;// 你的github用户名，非昵称$ git config --global user.email &quot;zhaoqian123@outlook.com&quot;// 填写你的github注册邮箱(输入完这两个后是啥都没有的)接着我们在电脑上生成密钥并将它添加到Github上边，$ ssh-keygen -t rsa -C &quot;你的github注册邮件地址&quot;用这个来实现本地与远程仓库的连接。第一次的时候密钥会直接显示，复制下来即可。如果已经生成过，可以按照它的提示在自己电脑上找到同样，这里也是将它复制下来。在自己Github的settings里找到SSh and GPG keys项，在SSH keys里加入刚刚复制的密钥(title可以随意起一个)在这里可以先复制好自己的仓库的地址 在hexo里配置进入hexo的配置文件，找到depoly字段，配置如下 远程部署在博客根目录打开Git Bash输入hexo g,hexo d这里第二个命令可能会出现问题，大概就是说找不到git，这里我们安装一下插件就可以解决$ npm install hexo-deployer-git --save,安装插件后在输入hexo d即可部署完成。上传文章在博客的根目录里打开GitBash，输入hexo n 你文章的名字即可创建新的文章，这个在source/_posts里可以看到 至此，博客搭建完毕。主题除了next也可以使用其他的，也可在hexo的官方文档里的主题里选择。]]></content>
      <categories>
        <category>总结</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
</search>
